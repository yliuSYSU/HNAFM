{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy import io\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from concurrent import futures\n",
    "import time\n",
    "import warnings\n",
    "from scipy import io\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "def train_test_split(matrix, ratio=0.2):\n",
    "    shape_row, shape_col = matrix.shape\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    for i in tqdm(range(shape_row)):\n",
    "        nonzero = matrix[i].nonzero()[1]\n",
    "        sample_len = int(len(nonzero) * ratio)\n",
    "        sample = np.random.choice(nonzero, sample_len, replace=False)\n",
    "        row.extend([i] * sample_len)\n",
    "        col.extend(sample)\n",
    "        data.extend([1] * sample_len)\n",
    "    test_matrix = sparse.csr_matrix((data, (row, col)), shape=(shape_row, shape_col))\n",
    "    train_matrix = matrix - test_matrix\n",
    "    return test_matrix, train_matrix\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 100/410 [20:29<1:03:30, 12.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-364.3842147941423\n",
      "time:  911\n",
      "rmse:  [0.73650969]\n",
      "mae:  [0.57710008]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 200/410 [56:38<59:28, 16.99s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-487.40030336283667\n",
      "time:  899\n",
      "rmse:  [0.73349738]\n",
      "mae:  [0.57545361]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 300/410 [1:32:11<33:48, 18.44s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-473.1459140560952\n",
      "time:  888\n",
      "rmse:  [0.73166543]\n",
      "mae:  [0.57461425]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 400/410 [2:07:46<03:11, 19.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-390.26265632095703\n",
      "time:  918\n",
      "rmse:  [0.72825548]\n",
      "mae:  [0.56844718]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [2:25:10<00:00, 21.25s/it]\n"
     ]
    }
   ],
   "source": [
    "douban_mat = io.loadmat('douban.mat')\n",
    "matrix_UM = douban_mat['relation'][0, 0]\n",
    "matrix_UU = douban_mat['relation'][0, 1]\n",
    "matrix_UG = douban_mat['relation'][0, 2]\n",
    "matrix_UL = douban_mat['relation'][0, 3]\n",
    "matrix_MD = douban_mat['relation'][0, 4]\n",
    "matrix_MA = douban_mat['relation'][0, 5]\n",
    "matrix_MT = douban_mat['relation'][0, 6]\n",
    "\n",
    "test = load_obj('douban/20/4/test.mtx')\n",
    "train = load_obj('douban/20/4/train.mtx')\n",
    "matrix_UGUM = matrix_UG * matrix_UG.T * train\n",
    "matrix_UMUM = train * train.T * train\n",
    "matrix_UMDMUM = train * matrix_MD * matrix_MD.T * train.T * train\n",
    "matrix_UMAMUM = train * matrix_MA * matrix_MA.T * train.T * train\n",
    "matrix_UMTMUM = train * matrix_MT * matrix_MT.T * train.T * train\n",
    "\n",
    "user_shape, item_shape = train.shape\n",
    "train_row, train_col = train.nonzero()\n",
    "test_row, test_col = test.nonzero()\n",
    "\n",
    "train_rates = np.array(matrix_UM[train_row, train_col])[0]\n",
    "test_rates = np.array(matrix_UM[test_row, test_col])[0]\n",
    "\n",
    "train_size = len(train_row)\n",
    "test_size = len(test_row)\n",
    "\n",
    "batchs = np.array(range(train_size))\n",
    "np.random.seed(2017)\n",
    "np.random.shuffle(batchs)\n",
    "\n",
    "batch_size = 10000\n",
    "begin = 0\n",
    "end = batch_size\n",
    "\n",
    "matrix_UGUM = matrix_UGUM.todense()\n",
    "matrix_UMUM = matrix_UMUM.todense()\n",
    "matrix_UMDMUM = matrix_UMDMUM.todense()\n",
    "matrix_UMAMUM = matrix_UMAMUM.todense()\n",
    "matrix_UMTMUM = matrix_UMTMUM.todense()\n",
    "\n",
    "tf_UGUM = tf.placeholder(tf.float64, [user_shape, item_shape])\n",
    "tf_UMUM = tf.placeholder(tf.float64, [user_shape, item_shape])\n",
    "tf_UMDMUM = tf.placeholder(tf.float64, [user_shape, item_shape])\n",
    "tf_UMAMUM = tf.placeholder(tf.float64, [user_shape, item_shape])\n",
    "tf_UMTMUM = tf.placeholder(tf.float64, [user_shape, item_shape])\n",
    "\n",
    "tensor_train_rows = tf.placeholder(tf.int32, [None])\n",
    "tensor_train_cols = tf.placeholder(tf.int32, [None])\n",
    "truth_index = tf.placeholder(tf.int32, [None])\n",
    "test_truth_index = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "train_true_rates = tf.expand_dims(tf.constant(train_rates), 1)\n",
    "test_true_rates = tf.expand_dims(tf.constant(test_rates), 1)\n",
    "\n",
    "user_input_UMUM = tf.nn.embedding_lookup(tf_UMUM, tensor_train_rows)\n",
    "item_input_UMUM = tf.nn.embedding_lookup(tf.transpose(tf_UMUM), tensor_train_cols)\n",
    "\n",
    "user_input_UGUM = tf.nn.embedding_lookup(tf_UGUM, tensor_train_rows)\n",
    "item_input_UGUM = tf.nn.embedding_lookup(tf.transpose(tf_UGUM), tensor_train_cols)\n",
    "\n",
    "user_input_UMDMUM = tf.nn.embedding_lookup(tf_UMDMUM, tensor_train_rows)\n",
    "item_input_UMDMUM = tf.nn.embedding_lookup(tf.transpose(tf_UMDMUM), tensor_train_cols)\n",
    "\n",
    "user_input_UMAMUM = tf.nn.embedding_lookup(tf_UMAMUM, tensor_train_rows)\n",
    "item_input_UMAMUM = tf.nn.embedding_lookup(tf.transpose(tf_UMAMUM), tensor_train_cols)\n",
    "\n",
    "user_input_UMTMUM = tf.nn.embedding_lookup(tf_UMTMUM, tensor_train_rows)\n",
    "item_input_UMTMUM = tf.nn.embedding_lookup(tf.transpose(tf_UMTMUM), tensor_train_cols)\n",
    "\n",
    "n_hidden_1 = 128\n",
    "n_embedding = 64\n",
    "\n",
    "\n",
    "user_embedding_UMUM = tf.layers.dense(tf.layers.dense(user_input_UMUM, units=n_hidden_1, activation=tf.nn.tanh), units=n_embedding, activation=tf.nn.tanh)\n",
    "user_embedding_UGUM = tf.layers.dense(tf.layers.dense(user_input_UGUM, units=n_hidden_1, activation=tf.nn.tanh), units=n_embedding, activation=tf.nn.tanh)             \n",
    "user_embedding_UMDMUM = tf.layers.dense(tf.layers.dense(user_input_UMDMUM, units=n_hidden_1, activation=tf.nn.tanh), units=n_embedding, activation=tf.nn.tanh)                             \n",
    "user_embedding_UMAMUM = tf.layers.dense(tf.layers.dense(user_input_UMAMUM, units=n_hidden_1, activation=tf.nn.tanh), units=n_embedding, activation=tf.nn.tanh)                          \n",
    "user_embedding_UMTMUM = tf.layers.dense(tf.layers.dense(user_input_UMTMUM, units=n_hidden_1, activation=tf.nn.tanh), units=n_embedding, activation=tf.nn.tanh)\n",
    "\n",
    "item_embedding_UMUM = tf.layers.dense(tf.layers.dense(item_input_UMUM, units=n_hidden_1, activation=tf.nn.tanh), units=n_embedding, activation=tf.nn.tanh)                                     \n",
    "item_embedding_UGUM = tf.layers.dense(tf.layers.dense(item_input_UGUM, units=n_hidden_1, activation=tf.nn.tanh), units=n_embedding, activation=tf.nn.tanh)                                   \n",
    "item_embedding_UMDMUM = tf.layers.dense(tf.layers.dense(item_input_UMDMUM, units=n_hidden_1, activation=tf.nn.tanh), units=n_embedding, activation=tf.nn.tanh)\n",
    "item_embedding_UMAMUM = tf.layers.dense(tf.layers.dense(item_input_UMAMUM, units=n_hidden_1, activation=tf.nn.tanh), units=n_embedding, activation=tf.nn.tanh)                                      \n",
    "item_embedding_UMTMUM = tf.layers.dense(tf.layers.dense(item_input_UMTMUM, units=n_hidden_1, activation=tf.nn.tanh), units=n_embedding, activation=tf.nn.tanh)\n",
    "\n",
    "W_u_init = tf.placeholder(tf.float64, [user_shape, n_embedding])\n",
    "W_i_init = tf.placeholder(tf.float64, [item_shape, n_embedding])\n",
    "\n",
    "W_u = tf.Variable(W_u_init)\n",
    "W_i = tf.Variable(W_i_init)\n",
    "\n",
    "W_u_train = tf.nn.embedding_lookup(W_u, tensor_train_rows)\n",
    "W_i_train = tf.nn.embedding_lookup(W_i, tensor_train_cols)\n",
    "\n",
    "user_input = tf.stack((user_embedding_UGUM, user_embedding_UMUM, user_embedding_UMDMUM, user_embedding_UMAMUM, user_embedding_UMTMUM))\n",
    "item_input = tf.stack((item_embedding_UGUM, item_embedding_UMUM, item_embedding_UMDMUM, item_embedding_UMAMUM, item_embedding_UMTMUM))\n",
    "\n",
    "user_val = tf.transpose(tf.nn.softmax(tf.transpose(tf.reduce_sum(tf.multiply(user_input, tf.expand_dims(W_u_train, 0)), 2))))\n",
    "item_val = tf.transpose(tf.nn.softmax(tf.transpose(tf.reduce_sum(tf.multiply(item_input, tf.expand_dims(W_i_train, 0)), 2))))\n",
    "\n",
    "user_feature = tf.reduce_sum(tf.multiply(tf.expand_dims(user_val, 2), user_input), 0)\n",
    "item_feature = tf.reduce_sum(tf.multiply(tf.expand_dims(item_val, 2), item_input), 0)\n",
    "\n",
    "FM_input = tf.stack((user_feature, item_feature))\n",
    "W_ui = tf.Variable(np.zeros([1, n_embedding]))\n",
    "input_val = tf.transpose(tf.nn.softmax(tf.transpose(tf.reduce_sum(tf.multiply(FM_input, tf.expand_dims(W_ui, 0)), 2))))\n",
    "user_items = tf.reduce_sum(tf.multiply(tf.expand_dims(input_val, 2), FM_input), 0)\n",
    "\n",
    "# D = n_embedding * 2\n",
    "D = n_embedding\n",
    "F = 30\n",
    "\n",
    "Bias_fm = tf.Variable(np.zeros([1]))\n",
    "# W_fm = tf.Variable(np.zeros([D, 1]))\n",
    "# V = tf.Variable(np.zeros([D, F]))\n",
    "W_fm = tf.Variable(tf.random_normal([D, 1], stddev=0.001, dtype=tf.float64, seed=2018))\n",
    "V = tf.Variable(tf.random_normal([D, F], stddev=0.001, dtype=tf.float64, seed=2018))\n",
    "\n",
    "reg = tf.nn.l2_loss(Bias_fm) + tf.nn.l2_loss(W_fm) + tf.nn.l2_loss(V)\n",
    "\n",
    "FM_res = tf.matmul(user_items, W_fm) + 0.5 * tf.expand_dims(tf.reduce_sum(tf.matmul(user_items, V), 1), 1) \\\n",
    "            - 0.5 * tf.expand_dims(tf.reduce_sum(tf.matmul(tf.square(user_items), tf.square(V)), 1), 1) + tf.expand_dims(Bias_fm, 1)\n",
    "ground_truth = tf.nn.embedding_lookup(train_true_rates, truth_index)\n",
    "loss = tf.reduce_sum(tf.square(FM_res - ground_truth), 0) \n",
    "gds = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "t_labels = tf.nn.embedding_lookup(test_true_rates, test_truth_index)\n",
    "rmse = tf.reduce_sum(tf.square(FM_res - t_labels), 0)\n",
    "mae = tf.reduce_sum(tf.abs(FM_res - t_labels), 0)\n",
    "\n",
    "u_init = np.zeros([user_shape, n_embedding]).astype(np.float64)\n",
    "i_init = np.zeros([item_shape, n_embedding]).astype(np.float64)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer(), {W_u_init: u_init, W_i_init: i_init})\n",
    "\n",
    "for _ in tqdm(range(410)):\n",
    "    if _ % 100 == 0 and _ >= 100:\n",
    "        df = pd.DataFrame(sess.run(W_u))\n",
    "        print (df.sum().sum())\n",
    "        err = 0\n",
    "        merr = 0\n",
    "        test_len = len(test_row)\n",
    "        split = []\n",
    "        test_round = 30\n",
    "        starttime = datetime.datetime.now()\n",
    "        for i in range(test_round + 1):\n",
    "            split.append(int(test_len * i / test_round))\n",
    "        for i in range(test_round):\n",
    "            test_sample_rows = np.array(test_row[split[i]:split[i + 1]]).astype(np.int)\n",
    "            test_sample_cols = np.array(test_col[split[i]:split[i + 1]]).astype(np.int)\n",
    "            test_sample = np.array(range(split[i], split[i + 1])).astype(np.int)\n",
    "            targs = {\n",
    "                tf_UMDMUM: matrix_UMDMUM,\n",
    "                tf_UMAMUM: matrix_UMAMUM,\n",
    "                tf_UMTMUM: matrix_UMTMUM,\n",
    "                tf_UGUM: matrix_UGUM,\n",
    "                tf_UMUM: matrix_UMUM,\n",
    "                tensor_train_rows: test_sample_rows, \n",
    "                tensor_train_cols: test_sample_cols, \n",
    "                test_truth_index: test_sample\n",
    "            }\n",
    "            err += sess.run(rmse, targs)\n",
    "            merr += sess.run(mae, targs)\n",
    "        endtime = datetime.datetime.now()\n",
    "        print ('time: ', (endtime - starttime).seconds)\n",
    "        print ('rmse: ', np.sqrt(err / test_size))\n",
    "        print ('mae: ', merr / test_size)\n",
    "    sample_index = batchs[begin:end]\n",
    "    sample_rows = np.array(train_row[sample_index]).astype(np.int)\n",
    "    sample_cols = np.array(train_col[sample_index]).astype(np.int)\n",
    "    if end + batch_size > train_size:\n",
    "        begin = 0\n",
    "        end = batch_size\n",
    "    else:\n",
    "        begin += batch_size\n",
    "        end += batch_size\n",
    "    args = {\n",
    "        tf_UMDMUM: matrix_UMDMUM,\n",
    "        tf_UMAMUM: matrix_UMAMUM,\n",
    "        tf_UMTMUM: matrix_UMTMUM,\n",
    "        tf_UGUM: matrix_UGUM,\n",
    "        tf_UMUM: matrix_UMUM,\n",
    "        tensor_train_rows: sample_rows,\n",
    "        tensor_train_cols: sample_cols,\n",
    "        truth_index: sample_index\n",
    "    }\n",
    "    l = sess.run(gds, args)\n",
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
